{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe343ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env LC_ALL=C.UTF-8\n",
    "%env LANG=C.UTF-8\n",
    "%env TRANSFORMERS_CACHE=cache\n",
    "%env HF_DATASETS_CACHE=cache\n",
    "%env CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb2d940",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torchaudio\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b89157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to specify the input and output column\n",
    "input_column = \"path\"\n",
    "output_despair_column = \"despair\"\n",
    "output_sadness_column = \"sadness\"\n",
    "output_pain_column = \"pain\"\n",
    "output_guilt_column = \"guilt\"\n",
    "output_confuse_column = \"confuse\"\n",
    "output_helplessness_column = \"helplessness\"\n",
    "output_resentment_column = \"resentment\"\n",
    "output_fear_column = \"fear\"\n",
    "output_numbness_column = \"numbness\"\n",
    "output_anxiety_column = \"anxiety\"\n",
    "output_grievance_column = \"grievance\"\n",
    "\n",
    "key_list = [\n",
    "    output_despair_column,\n",
    "    output_sadness_column,\n",
    "    output_pain_column,\n",
    "    output_guilt_column,\n",
    "    output_confuse_column,\n",
    "    output_helplessness_column,\n",
    "    output_resentment_column,\n",
    "    output_fear_column,\n",
    "    output_numbness_column,\n",
    "    output_anxiety_column,\n",
    "    output_grievance_column\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e45a26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to distinguish the unique labels in our SER dataset\n",
    "label_list = [0, 1]\n",
    "num_labels = len(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b385eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !source /etc/network_turbo\n",
    "from transformers import AutoConfig, Wav2Vec2Processor\n",
    "from transformers import AutoConfig, AutoFeatureExtractor\n",
    "model_name_or_path = \"../models/wav2vec2-large-xlsr-53-chinese-zh-cn\"\n",
    "pooling_mode = \"mean\"\n",
    "\n",
    "# config\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    num_labels=num_labels,\n",
    "    label2id={label: i for i, label in enumerate(label_list)},\n",
    "    id2label={i: label for i, label in enumerate(label_list)},\n",
    "    finetuning_task=\"wav2vec2_clf\",\n",
    ")\n",
    "setattr(config, 'pooling_mode', pooling_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e9be09",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = Wav2Vec2Processor.from_pretrained(model_name_or_path,)\n",
    "target_sampling_rate = processor.feature_extractor.sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23a4157",
   "metadata": {},
   "outputs": [],
   "source": [
    "def speech_file_to_array_fn(path):\n",
    "    speech_array, sampling_rate = torchaudio.load(path)\n",
    "    resampler = torchaudio.transforms.Resample(sampling_rate, target_sampling_rate)\n",
    "    speech = resampler(speech_array).squeeze().numpy()\n",
    "    return speech\n",
    "\n",
    "def label_to_id(label, label_list):\n",
    "    if len(label_list) > 0:\n",
    "        return label_list.index(label) if label in label_list else -1\n",
    "\n",
    "    return label\n",
    "\n",
    "def label_to_id_float(label, label_list):\n",
    "    return float(label_to_id(label, label_list))\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    speech_list = [speech_file_to_array_fn(path) for path in examples[input_column]]\n",
    "    target_list = []\n",
    "    for output_column in key_list:\n",
    "        single_col = [label_to_id(label, label_list) for label in examples[output_column]]\n",
    "        target_list.append(single_col)\n",
    "\n",
    "    target_list = np.array(target_list).transpose()\n",
    "    \n",
    "    res = {}\n",
    "    result = processor(speech_list, sampling_rate=target_sampling_rate)\n",
    "    res['input_values'] = result.input_values\n",
    "    res[\"labels\"] = target_list\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e70505",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple\n",
    "import torch\n",
    "from transformers.file_utils import ModelOutput\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SpeechClassifierOutput(ModelOutput):\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    logits: torch.FloatTensor = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9047a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "\n",
    "# from transformers.models.whisper.modeling_whisper import WhisperModel\n",
    "\n",
    "\n",
    "from transformers.models.wav2vec2.modeling_wav2vec2 import (\n",
    "    Wav2Vec2PreTrainedModel,\n",
    "    Wav2Vec2Model\n",
    ")    \n",
    "\n",
    "# 这个头部模块负责将从Whisper模型的特征中提取的音频表示转化为用于进行分类任务的输出\n",
    "# 这个头部模块负责将从Wav2Vec2模型的特征中提取的音频表示转化为用于进行分类任务的输出\n",
    "class Wav2Vec2ClassificationHead(nn.Module):\n",
    "    \"\"\"Head for wav2vec classification task.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.final_dropout)\n",
    "        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "    # 这个方法定义了数据在模块中的前向传播过程。\n",
    "    # 它首先通过self.dropout应用丢弃以随机丢弃一些特征，然后通过self.dense进行线性映射\n",
    "    # 并应用tanh激活函数，最后通过self.out_proj进行线性映射，以生成分类任务的输出分数。\n",
    "    def forward(self, features, **kwargs):\n",
    "        x = features\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out_proj(x)\n",
    "        return x\n",
    "\n",
    "# 将Wav2Vec2的特征提取器和分类头部组合在一起，以创建一个端到端的语音分类模型。\n",
    "class Wav2Vec2ForSpeechClassification(Wav2Vec2PreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.pooling_mode = config.pooling_mode\n",
    "        self.config = config\n",
    "\n",
    "        self.wav2vec2 = Wav2Vec2Model(config)\n",
    "        #self.classifier = Wav2Vec2ClassificationHead(config)\n",
    "        self.despair_classifier = Wav2Vec2ClassificationHead(config)\n",
    "        self.sadness_classifier = Wav2Vec2ClassificationHead(config)\n",
    "        self.pain_classifier = Wav2Vec2ClassificationHead(config)\n",
    "        self.guilt_classifier = Wav2Vec2ClassificationHead(config)\n",
    "        self.confuse_classifier = Wav2Vec2ClassificationHead(config)\n",
    "        self.helplessness_classifier = Wav2Vec2ClassificationHead(config)\n",
    "        self.resentment_classifier = Wav2Vec2ClassificationHead(config)\n",
    "        self.fear_classifier = Wav2Vec2ClassificationHead(config)\n",
    "        self.numbness_classifier = Wav2Vec2ClassificationHead(config)\n",
    "        self.anxiety_classifier = Wav2Vec2ClassificationHead(config)\n",
    "        self.grievance_classifier = Wav2Vec2ClassificationHead(config)\n",
    "        \n",
    "\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def freeze_feature_extractor(self):\n",
    "        self.wav2vec2.feature_extractor._freeze_parameters()\n",
    "\n",
    "    def merged_strategy(\n",
    "            self,\n",
    "            hidden_states,\n",
    "            mode=\"mean\"\n",
    "    ):\n",
    "        if mode == \"mean\":\n",
    "            outputs = torch.mean(hidden_states, dim=1)\n",
    "        elif mode == \"sum\":\n",
    "            outputs = torch.sum(hidden_states, dim=1)\n",
    "        elif mode == \"max\":\n",
    "            outputs = torch.max(hidden_states, dim=1)[0]\n",
    "        else:\n",
    "            raise Exception(\n",
    "                \"The pooling method hasn't been defined! Your pooling mode must be one of these ['mean', 'sum', 'max']\")\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            input_values,\n",
    "            attention_mask=None,\n",
    "            output_attentions=None,\n",
    "            output_hidden_states=None,\n",
    "            return_dict=None,\n",
    "            labels=None,\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        outputs = self.wav2vec2(\n",
    "            input_values,\n",
    "            attention_mask=attention_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        hidden_states = outputs[0]\n",
    "        hidden_states = self.merged_strategy(hidden_states, mode=self.pooling_mode)\n",
    "        #logits = self.classifier(hidden_states)\n",
    "        \n",
    "        despair_logits = self.despair_classifier(hidden_states).unsqueeze(1)\n",
    "        sadness_logits = self.sadness_classifier(hidden_states).unsqueeze(1)\n",
    "        pain_logits = self.pain_classifier(hidden_states).unsqueeze(1)\n",
    "        guilt_logits = self.guilt_classifier(hidden_states).unsqueeze(1)\n",
    "        confuse_logits = self.confuse_classifier(hidden_states).unsqueeze(1)\n",
    "        helplessness_logits = self.helplessness_classifier(hidden_states).unsqueeze(1)\n",
    "        resentment_logits = self.resentment_classifier(hidden_states).unsqueeze(1)\n",
    "        fear_logits = self.fear_classifier(hidden_states).unsqueeze(1)\n",
    "        numbness_logits = self.numbness_classifier(hidden_states).unsqueeze(1)\n",
    "        anxiety_logits = self.anxiety_classifier(hidden_states).unsqueeze(1)\n",
    "        grievance_logits = self.grievance_classifier(hidden_states).unsqueeze(1)\n",
    "        \n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            despair_loss = loss_fct(despair_logits.view(-1, self.num_labels), labels[:, 0])\n",
    "            sadness_loss = loss_fct(sadness_logits.view(-1, self.num_labels), labels[:, 1])\n",
    "            pain_loss = loss_fct(pain_logits.view(-1, self.num_labels), labels[:, 2])\n",
    "            guilt_loss = loss_fct(guilt_logits.view(-1, self.num_labels), labels[:, 3])\n",
    "            confuse_loss = loss_fct(confuse_logits.view(-1, self.num_labels), labels[:, 4])\n",
    "            helplessness_loss = loss_fct(helplessness_logits.view(-1, self.num_labels), labels[:, 5])\n",
    "            resentment_loss = loss_fct(resentment_logits.view(-1, self.num_labels), labels[:, 6])\n",
    "            fear_loss = loss_fct(fear_logits.view(-1, self.num_labels), labels[:, 7])\n",
    "            numbness_loss = loss_fct(numbness_logits.view(-1, self.num_labels), labels[:, 8])\n",
    "            anxiety_loss = loss_fct(anxiety_logits.view(-1, self.num_labels), labels[:, 9])\n",
    "            grievance_loss = loss_fct(grievance_logits.view(-1, self.num_labels), labels[:, 10])\n",
    "            \n",
    "            loss = despair_loss + \\\n",
    "                sadness_loss + \\\n",
    "                pain_loss + \\\n",
    "                guilt_loss + \\\n",
    "                confuse_loss + \\\n",
    "                helplessness_loss + \\\n",
    "                resentment_loss + \\\n",
    "                fear_loss + \\\n",
    "                numbness_loss + \\\n",
    "                anxiety_loss + \\\n",
    "                grievance_loss\n",
    "        \n",
    "        output = torch.concat((\n",
    "            despair_logits,\n",
    "            sadness_logits,\n",
    "            pain_logits,\n",
    "            guilt_logits,\n",
    "            confuse_logits,\n",
    "            helplessness_logits,\n",
    "            resentment_logits,\n",
    "            fear_logits,\n",
    "            numbness_logits,\n",
    "            anxiety_logits,\n",
    "            grievance_logits),\n",
    "            dim=1\n",
    "        )\n",
    "        return {'loss': loss, 'output': output} if loss is not None else output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83f5dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Union\n",
    "import torch\n",
    "\n",
    "import transformers\n",
    "from transformers import Wav2Vec2Processor\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
    "            The processor used for proccessing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "        max_length (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n",
    "        max_length_labels (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``labels`` returned list and optionally padding length (see above).\n",
    "        pad_to_multiple_of (:obj:`int`, `optional`):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
    "            7.5 (Volta).\n",
    "    \"\"\"\n",
    "\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: Optional[int] = None\n",
    "    max_length_labels: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    pad_to_multiple_of_labels: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [feature[\"labels\"] for feature in features]\n",
    "\n",
    "        d_type = torch.long\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        batch[\"labels\"] = torch.tensor(label_features, dtype=d_type)\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3939763b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c3c3ae",
   "metadata": {},
   "source": [
    "Next, the evaluation metric is defined. There are many pre-defined metrics for classification/regression problems, but in this case, we would continue with just Accuracy for classification and MSE for regression. You can define other metrics on your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c5bbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import EvalPrediction\n",
    "from sklearn.metrics import f1_score, recall_score\n",
    "\n",
    "is_regression = False\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    if isinstance(p.predictions, dict):\n",
    "        preds = p.predictions['output']\n",
    "    elif isinstance(p.predictions, tuple) or isinstance(p.predictions, list):\n",
    "        if len(p.predictions) == 1:\n",
    "            preds = p.predictions[0]\n",
    "        else:\n",
    "            preds = p.predictions[1]\n",
    "    else:\n",
    "        preds = p.predictions\n",
    "    preds = np.squeeze(preds) if is_regression else np.argmax(preds, axis=2)\n",
    "\n",
    "    if is_regression:\n",
    "        mse = ((preds - p.label_ids) ** 2).mean().item()\n",
    "        return {\"mse\": mse}\n",
    "    else:\n",
    "        accuracy = (preds == p.label_ids).astype(np.float32).mean().item()\n",
    "        \n",
    "        # f1 = f1_score(p.label_ids, preds, average='weighted')\n",
    "        # recall = recall_score(p.label_ids, preds, average='weighted')\n",
    "        \n",
    "        f1 = 0\n",
    "        recall = 0\n",
    "        for i in range(preds.shape[-1]):\n",
    "            f1 += f1_score(p.label_ids[:, i], preds[:, i], average='weighted')\n",
    "            recall += recall_score(p.label_ids[:, i], preds[:, i], average='weighted')\n",
    "        f1 /= preds.shape[-1]\n",
    "        recall /= preds.shape[-1]\n",
    "        return {\"accuracy\": accuracy, \"f1\": f1, \"recall\": recall}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d377d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Wav2Vec2ForSpeechClassification.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    config=config,\n",
    ")\n",
    "model.freeze_feature_extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789fdae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../outputs_multi/wav2vec2_output\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    num_train_epochs=1.0,\n",
    "    fp16=True,\n",
    "    save_steps=50,\n",
    "    eval_steps=25,\n",
    "    logging_steps=10,\n",
    "    learning_rate=1e-4,\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7e4422",
   "metadata": {},
   "source": [
    "For future use we can create our training script, we do it in a simple way. You can add more on you own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6716c1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Union\n",
    "\n",
    "import torch\n",
    "from packaging import version\n",
    "from torch import nn\n",
    "\n",
    "from transformers import (\n",
    "    Trainer,\n",
    "    is_apex_available,\n",
    ")\n",
    "\n",
    "if is_apex_available():\n",
    "    from apex import amp\n",
    "\n",
    "if version.parse(torch.__version__) >= version.parse(\"1.6\"):\n",
    "    _is_native_amp_available = True\n",
    "    from torch.cuda.amp import autocast\n",
    "\n",
    "\n",
    "class CTCTrainer(Trainer):\n",
    "    def training_step(self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Perform a training step on a batch of inputs.\n",
    "\n",
    "        Subclass and override to inject custom behavior.\n",
    "\n",
    "        Args:\n",
    "            model (:obj:`nn.Module`):\n",
    "                The model to train.\n",
    "            inputs (:obj:`Dict[str, Union[torch.Tensor, Any]]`):\n",
    "                The inputs and targets of the model.\n",
    "\n",
    "                The dictionary will be unpacked before being fed to the model. Most models expect the targets under the\n",
    "                argument :obj:`labels`. Check your model's documentation for all accepted arguments.\n",
    "\n",
    "        Return:\n",
    "            :obj:`torch.Tensor`: The tensor with training loss on this batch.\n",
    "        \"\"\"\n",
    "        model.train()\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "\n",
    "        with self.compute_loss_context_manager():\n",
    "            loss = self.compute_loss(model, inputs)\n",
    "        \n",
    "        if self.args.gradient_accumulation_steps > 1:\n",
    "            loss = loss / self.args.gradient_accumulation_steps\n",
    "\n",
    "        if self.use_apex:\n",
    "            with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n",
    "                scaled_loss.backward()\n",
    "        else:\n",
    "            self.accelerator.backward(loss)\n",
    "\n",
    "        return loss.detach()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad20061a",
   "metadata": {},
   "source": [
    "Now, all instances can be passed to Trainer and we are ready to start training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5e70d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_train_data = pd.read_csv('../data/total.csv')\n",
    "eval_data_num = int(len(total_train_data) / 5)\n",
    "for i in range(5):\n",
    "    if i == 0:\n",
    "        eval_data = total_train_data.iloc[:eval_data_num, :]\n",
    "        train_data = total_train_data.iloc[eval_data_num:, :]\n",
    "    elif i == 4:\n",
    "        eval_data = total_train_data.iloc[-eval_data_num:, :]\n",
    "        train_data = total_train_data.iloc[:-eval_data_num, :]\n",
    "    else:\n",
    "        eval_data = total_train_data.iloc[i * eval_data_num : (i + 1) * eval_data_num, :]\n",
    "        train_data = pd.concat([total_train_data.iloc[:i * eval_data_num, :], total_train_data.iloc[(i + 1) * eval_data_num:, :]], axis=0, ignore_index=True).sample(frac=1)\n",
    "\n",
    "    eval_data.to_csv('../data/5fold_eval_{}.csv'.format(i), index=False)\n",
    "    train_data.to_csv('../data/5fold_train_{}.csv'.format(i), index=False)\n",
    "    \n",
    "    data_files = {\n",
    "        \"train\": \"../data/5fold_train_{}.csv\".format(i),\n",
    "        \"validation\": \"../data/5fold_eval_{}.csv\".format(i),\n",
    "    }\n",
    "\n",
    "    dataset = load_dataset(\"csv\", data_files=data_files, delimiter=\",\", )\n",
    "    train_dataset = dataset[\"train\"]\n",
    "    eval_dataset = dataset[\"validation\"]\n",
    "\n",
    "    train_dataset = train_dataset.map(\n",
    "        preprocess_function,\n",
    "        batch_size=5,\n",
    "        batched=True,\n",
    "        drop_last_batch=False\n",
    "    )\n",
    "    eval_dataset = eval_dataset.map(\n",
    "        preprocess_function,\n",
    "        batch_size=5,\n",
    "        batched=True,\n",
    "        drop_last_batch=False\n",
    "    )\n",
    "\n",
    "    trainer = CTCTrainer(\n",
    "        model=model,\n",
    "        data_collator=data_collator,\n",
    "        args=training_args,\n",
    "        compute_metrics=compute_metrics,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=processor.feature_extractor,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    metrics = trainer.evaluate()\n",
    "    max_eval_samples = len(eval_dataset)\n",
    "    metrics[\"eval_samples\"] = min(max_eval_samples, len(eval_dataset))\n",
    "\n",
    "    trainer.log_metrics(\"eval\", metrics)\n",
    "    trainer.save_metrics(\"eval\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c764b4a8",
   "metadata": {},
   "source": [
    "测试代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51353730",
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors import safe_open\n",
    "\n",
    "# 加载模型\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained('../outputs_multi/wav2vec2_output/checkpoint-1650')\n",
    "target_sampling_rate = feature_extractor.sampling_rate\n",
    "model = Wav2Vec2ForSpeechClassification(config)\n",
    "\n",
    "tensors = {}\n",
    "with safe_open('../outputs_multi/wav2vec2_output/checkpoint-1650/model.safetensors', framework=\"pt\", device='cpu') as f:\n",
    "    for k in f.keys():\n",
    "        tensors[k] = f.get_tensor(k)\n",
    "model.load_state_dict(tensors)\n",
    "\n",
    "# 读取测试集\n",
    "data_files = {\n",
    "    \"test\": \"../data/test.csv\"\n",
    "}\n",
    "dataset = load_dataset(\"csv\", data_files=data_files, delimiter=\",\", )\n",
    "test_dataset = dataset[\"test\"]\n",
    "test_dataset = test_dataset.map(\n",
    "    preprocess_function,\n",
    "    batch_size=100,\n",
    "    batched=True,\n",
    "    drop_last_batch=False\n",
    ")\n",
    "\n",
    "# 进行测试评估\n",
    "trainer = CTCTrainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=feature_extractor\n",
    ")\n",
    "\n",
    "metrics = trainer.evaluate()\n",
    "max_eval_samples = len(test_dataset)\n",
    "metrics[\"eval_samples\"] = min(max_eval_samples, len(test_dataset))\n",
    "\n",
    "trainer.log_metrics(\"eval\", metrics)\n",
    "trainer.save_metrics(\"eval\", metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:czl_py]",
   "language": "python",
   "name": "conda-env-czl_py-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
