{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe343ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def count_files_in_directory(directory):\n",
    "    total_files = 0\n",
    "\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        total_files += len(files)\n",
    "\n",
    "    return total_files\n",
    "\n",
    "path_all_data = 'data/all_data_binarylabel'  # 替换为你要统计的目录的路径\n",
    "\n",
    "total_files = count_files_in_directory(path_all_data)\n",
    "\n",
    "print(\"询问者语音数:\", total_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66745fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env LC_ALL=C.UTF-8\n",
    "%env LANG=C.UTF-8\n",
    "%env TRANSFORMERS_CACHE=cache\n",
    "%env HF_DATASETS_CACHE=cache\n",
    "%env CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a908d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torchaudio\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd97cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "i=0\n",
    "for path in tqdm(Path(\"data/all_data_binarylabel/\").glob(\"**/*.wav\")):\n",
    "    name = str(path).split('/')[-1].split('.')[0]\n",
    "    label = str(path).split('/')[-2]\n",
    "    # print('name:',name,'label:',label)\n",
    "    i=i+1\n",
    "\n",
    "    # 尝试使用torchaudio.load加载音频文件，果加载成功，将音频文件的名称、路径和情感标签存储为一个字典，并将该字典添加到data列表中\n",
    "    try:\n",
    "        # There are some broken files\n",
    "        s = torchaudio.load(path)\n",
    "        data.append({\n",
    "            \"name\": name,\n",
    "            \"path\": path,\n",
    "            \"emotion\": label\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(str(path), e)\n",
    "        pass\n",
    "\n",
    "print(len(data),i)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ec6247",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5713f446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter broken and non-existed paths\n",
    "\n",
    "print(f\"Step 0: {len(df)}\")\n",
    "\n",
    "df[\"status\"] = df[\"path\"].apply(lambda path: True if os.path.exists(path) else None)\n",
    "df = df.dropna(subset=[\"path\"])\n",
    "df = df.drop(labels=\"status\", axis=1)\n",
    "print(f\"Step 1: {len(df)}\")\n",
    "\n",
    "df = df.sample(frac=1)\n",
    "df = df.reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13f0a9d",
   "metadata": {},
   "source": [
    "Let's explore how many labels (emotions) are in the dataset with what distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85836557",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Labels: \", df[\"emotion\"].unique())\n",
    "print()\n",
    "df.groupby(\"emotion\").count()[[\"path\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c55386",
   "metadata": {},
   "source": [
    "Let's display some random sample of the dataset and run it a couple of times to get a feeling for the audio and the emotional label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd52d5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import librosa\n",
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "\n",
    "idx = np.random.randint(0, len(df))\n",
    "sample = df.iloc[idx]\n",
    "path = sample[\"path\"]\n",
    "label = sample[\"emotion\"]\n",
    "\n",
    "\n",
    "print(f\"ID Location: {idx}\")\n",
    "print(f\"      Label: {label}\")\n",
    "print()\n",
    "\n",
    "speech, sr = torchaudio.load(path)\n",
    "speech = speech[0].numpy().squeeze()\n",
    "speech = librosa.resample(np.asarray(speech),orig_sr= sr, target_sr=16_000)\n",
    "ipd.Audio(data=np.asarray(speech), autoplay=True, rate=16000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95daa20e",
   "metadata": {},
   "source": [
    "For training purposes, we need to split data into train test sets; in this specific example, we break with a 20% rate for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4851a5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"data/all_data_binarylabel\"\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=101, stratify=df[\"emotion\"])\n",
    "\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "train_df.to_csv(f\"{save_path}/train.csv\", sep=\"\\t\", encoding=\"utf-8\", index=False)\n",
    "test_df.to_csv(f\"{save_path}/test.csv\", sep=\"\\t\", encoding=\"utf-8\", index=False)\n",
    "\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523feba7",
   "metadata": {},
   "source": [
    "Prepare Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9975201b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the created dataset using datasets\n",
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "\n",
    "data_files = {\n",
    "    \"train\": \"data/all_data_binarylabel/train.csv\",\n",
    "    \"validation\": \"data/all_data_binarylabel/test.csv\",\n",
    "}\n",
    "\n",
    "dataset = load_dataset(\"csv\", data_files=data_files, delimiter=\"\\t\", )\n",
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset = dataset[\"validation\"]\n",
    "\n",
    "print(train_dataset)\n",
    "print(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b89157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to specify the input and output column\n",
    "input_column = \"path\"\n",
    "output_column = \"emotion\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e45a26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to distinguish the unique labels in our SER dataset\n",
    "label_list = train_dataset.unique(output_column)\n",
    "label_list.sort()  # Let's sort it for determinism\n",
    "num_labels = len(label_list)\n",
    "print(f\"A classification problem with {num_labels} classes: {label_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b385eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !source /etc/network_turbo\n",
    "\n",
    "from transformers import AutoConfig, AutoFeatureExtractor\n",
    "model_name_or_path = \"models/openai-whisper-large-v3\"\n",
    "pooling_mode = \"mean\"\n",
    "\n",
    "# config\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    num_labels=num_labels,\n",
    "    label2id={label: i for i, label in enumerate(label_list)},\n",
    "    id2label={i: label for i, label in enumerate(label_list)},\n",
    "    finetuning_task=\"whisper_clf\",\n",
    ")\n",
    "setattr(config, 'pooling_mode', pooling_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e9be09",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = AutoFeatureExtractor.from_pretrained(model_name_or_path,)\n",
    "target_sampling_rate = feature_extractor.sampling_rate\n",
    "print(f\"The target sampling rate: {target_sampling_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23a4157",
   "metadata": {},
   "outputs": [],
   "source": [
    "def speech_file_to_array_fn(path):\n",
    "    speech_array, sampling_rate = torchaudio.load(path)\n",
    "    resampler = torchaudio.transforms.Resample(sampling_rate, target_sampling_rate)\n",
    "    speech = resampler(speech_array).squeeze().numpy()\n",
    "    return speech\n",
    "\n",
    "def label_to_id(label, label_list):\n",
    "    if len(label_list) > 0:\n",
    "        return label_list.index(label) if label in label_list else -1\n",
    "\n",
    "    return label\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    speech_list = [speech_file_to_array_fn(path) for path in examples[input_column]]\n",
    "    target_list = [label_to_id(label, label_list) for label in examples[output_column]]\n",
    "\n",
    "    res = {}\n",
    "    result = feature_extractor(speech_list, return_tensors=\"pt\", sampling_rate=target_sampling_rate)\n",
    "    res['input_features'] = result.input_features\n",
    "    res[\"labels\"] = list(target_list)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8109c78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(\n",
    "    preprocess_function,\n",
    "    batch_size=5,\n",
    "    batched=True,\n",
    "    num_proc=4\n",
    ")\n",
    "eval_dataset = eval_dataset.map(\n",
    "    preprocess_function,\n",
    "    batch_size=5,\n",
    "    batched=True,\n",
    "    num_proc=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e9aac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 6\n",
    "print(len(train_dataset))\n",
    "print(f\"Training input_values: \", len(train_dataset[idx]['input_features']))\n",
    "print(f\"Training labels: {train_dataset[idx]['labels']} - {train_dataset[idx]['emotion']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de670ac9",
   "metadata": {},
   "source": [
    "Great, now we've successfully read all the audio files, resampled the audio files to 16kHz, and mapped each audio to the corresponding label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e70505",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple\n",
    "import torch\n",
    "from transformers.file_utils import ModelOutput\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SpeechClassifierOutput(ModelOutput):\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    logits: torch.FloatTensor = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9047a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "\n",
    "from transformers.models.whisper.modeling_whisper import WhisperModel, WhisperPreTrainedModel\n",
    "\n",
    "# 这个头部模块负责将从Whisper模型的特征中提取的音频表示转化为用于进行分类任务的输出\n",
    "class WhisperClassificationHead(nn.Module):\n",
    "    \"\"\"Head for wav2vec classification task.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.dropout = nn.Dropout(0.)\n",
    "        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "    # 这个方法定义了数据在模块中的前向传播过程。\n",
    "    # 它首先通过self.dropout应用丢弃以随机丢弃一些特征，然后通过self.dense进行线性映射\n",
    "    # 并应用tanh激活函数，最后通过self.out_proj进行线性映射，以生成分类任务的输出分数。\n",
    "    def forward(self, features, **kwargs):\n",
    "        x = features\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out_proj(x)\n",
    "        return x\n",
    "\n",
    "# 将Whisper的特征提取器和分类头部组合在一起，以创建一个端到端的语音分类模型。\n",
    "class WhisperForSpeechClassification(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.num_labels = config.num_labels\n",
    "        self.pooling_mode = config.pooling_mode\n",
    "        self.config = config\n",
    "\n",
    "        self.whisper = WhisperModel.from_pretrained(model_name_or_path)\n",
    "        self.classifier = WhisperClassificationHead(config)\n",
    "\n",
    "        self._init_weights(self)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        std = self.config.init_std\n",
    "        if isinstance(module, (nn.Linear, nn.Conv1d)):\n",
    "            module.weight.data.normal_(mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=std)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "\n",
    "    def freeze_feature_extractor(self):\n",
    "        self.whisper.encoder._freeze_parameters()\n",
    "\n",
    "    def merged_strategy(\n",
    "            self,\n",
    "            hidden_states,\n",
    "            mode=\"mean\"\n",
    "    ):\n",
    "        if mode == \"mean\":\n",
    "            outputs = torch.mean(hidden_states, dim=1)\n",
    "        elif mode == \"sum\":\n",
    "            outputs = torch.sum(hidden_states, dim=1)\n",
    "        elif mode == \"max\":\n",
    "            outputs = torch.max(hidden_states, dim=1)[0]\n",
    "        else:\n",
    "            raise Exception(\n",
    "                \"The pooling method hasn't been defined! Your pooling mode must be one of these ['mean', 'sum', 'max']\")\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            input_features,\n",
    "            labels=None\n",
    "    ):\n",
    "        decoder_input_ids = torch.ones([input_features.shape[0], 1], dtype=torch.long) * self.whisper.config.decoder_start_token_id\n",
    "        decoder_input_ids = decoder_input_ids.cuda()\n",
    "        hidden_states = self.whisper(input_features, decoder_input_ids=decoder_input_ids).last_hidden_state\n",
    "        hidden_states = self.merged_strategy(hidden_states, mode=self.pooling_mode)\n",
    "        logits = self.classifier(hidden_states)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.config.problem_type is None:\n",
    "                if self.num_labels == 1:\n",
    "                    self.config.problem_type = \"regression\"\n",
    "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
    "                    self.config.problem_type = \"single_label_classification\"\n",
    "                else:\n",
    "                    self.config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "            if self.config.problem_type == \"regression\":\n",
    "                loss_fct = MSELoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels)\n",
    "            elif self.config.problem_type == \"single_label_classification\":\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            elif self.config.problem_type == \"multi_label_classification\":\n",
    "                loss_fct = BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits, labels)\n",
    "        \n",
    "        output = (logits,)\n",
    "        return ((loss,) + output) if loss is not None else output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83f5dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Union\n",
    "import torch\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
    "            The processor used for proccessing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "        max_length (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n",
    "        max_length_labels (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``labels`` returned list and optionally padding length (see above).\n",
    "        pad_to_multiple_of (:obj:`int`, `optional`):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
    "            7.5 (Volta).\n",
    "    \"\"\"\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        input_features = [feature[\"input_features\"] for feature in features]\n",
    "        label_features = [feature[\"labels\"] for feature in features]\n",
    "\n",
    "        d_type = torch.long if isinstance(label_features[0], int) else torch.float\n",
    "\n",
    "        batch = {}\n",
    "        batch['input_features'] = torch.tensor(input_features, dtype=torch.half)\n",
    "        batch[\"labels\"] = torch.tensor(label_features, dtype=d_type)\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3939763b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorCTCWithPadding()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c3c3ae",
   "metadata": {},
   "source": [
    "Next, the evaluation metric is defined. There are many pre-defined metrics for classification/regression problems, but in this case, we would continue with just Accuracy for classification and MSE for regression. You can define other metrics on your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d9f251",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_regression = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c5bbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import EvalPrediction\n",
    "from sklearn.metrics import f1_score, recall_score\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if (isinstance(p.predictions, tuple) or isinstance(p.predictions, list)) else p.predictions\n",
    "    preds = np.squeeze(preds) if is_regression else np.argmax(preds, axis=1)\n",
    "\n",
    "    if is_regression:\n",
    "        mse = ((preds - p.label_ids) ** 2).mean().item()\n",
    "        return {\"mse\": mse}\n",
    "    else:\n",
    "        accuracy = (preds == p.label_ids).astype(np.float32).mean().item()\n",
    "        f1 = f1_score(p.label_ids, preds, average='weighted')\n",
    "        recall = recall_score(p.label_ids, preds, average='weighted')\n",
    "        print({\"accuracy\": accuracy, \"f1\": f1, \"recall\": recall})\n",
    "\n",
    "        return {\"accuracy\": accuracy, \"f1\": f1, \"recall\": recall}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d377d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WhisperForSpeechClassification(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150560b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.freeze_feature_extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789fdae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"outputs/all_data_whisper_large_output\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    num_train_epochs=1.0,\n",
    "    fp16=True,\n",
    "    save_steps=10,\n",
    "    eval_steps=10,\n",
    "    logging_steps=10,\n",
    "    learning_rate=1e-4,\n",
    "    save_total_limit=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7e4422",
   "metadata": {},
   "source": [
    "For future use we can create our training script, we do it in a simple way. You can add more on you own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6716c1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Union\n",
    "\n",
    "import torch\n",
    "from packaging import version\n",
    "from torch import nn\n",
    "\n",
    "from transformers import (\n",
    "    Trainer,\n",
    "    is_apex_available,\n",
    ")\n",
    "\n",
    "if is_apex_available():\n",
    "    from apex import amp\n",
    "\n",
    "if version.parse(torch.__version__) >= version.parse(\"1.6\"):\n",
    "    _is_native_amp_available = True\n",
    "    from torch.cuda.amp import autocast\n",
    "\n",
    "\n",
    "class CTCTrainer(Trainer):\n",
    "    def training_step(self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Perform a training step on a batch of inputs.\n",
    "\n",
    "        Subclass and override to inject custom behavior.\n",
    "\n",
    "        Args:\n",
    "            model (:obj:`nn.Module`):\n",
    "                The model to train.\n",
    "            inputs (:obj:`Dict[str, Union[torch.Tensor, Any]]`):\n",
    "                The inputs and targets of the model.\n",
    "\n",
    "                The dictionary will be unpacked before being fed to the model. Most models expect the targets under the\n",
    "                argument :obj:`labels`. Check your model's documentation for all accepted arguments.\n",
    "\n",
    "        Return:\n",
    "            :obj:`torch.Tensor`: The tensor with training loss on this batch.\n",
    "        \"\"\"\n",
    "        model.train()\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "\n",
    "        with self.compute_loss_context_manager():\n",
    "            loss = self.compute_loss(model, inputs)\n",
    "        \n",
    "        if self.args.gradient_accumulation_steps > 1:\n",
    "            loss = loss / self.args.gradient_accumulation_steps\n",
    "\n",
    "        if self.use_apex:\n",
    "            with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n",
    "                scaled_loss.backward()\n",
    "        else:\n",
    "            self.accelerator.backward(loss)\n",
    "\n",
    "        return loss.detach()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad20061a",
   "metadata": {},
   "source": [
    "Now, all instances can be passed to Trainer and we are ready to start training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5e70d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = CTCTrainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0be9cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d8fe31",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = trainer.evaluate()\n",
    "max_eval_samples = len(eval_dataset)\n",
    "metrics[\"eval_samples\"] = min(max_eval_samples, len(eval_dataset))\n",
    "\n",
    "trainer.log_metrics(\"eval\", metrics)\n",
    "trainer.save_metrics(\"eval\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c248801",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:czl_py]",
   "language": "python",
   "name": "conda-env-czl_py-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
